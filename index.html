<!DOCTYPE html>
<html>

<head>
<title>Shuwen Qiu</title>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<style>
	body {
		font: 14px/1.5 -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
		padding-left: 10%;
		padding-right: 10%;
	}

	header {
		border-bottom: 1px solid gray;
		text-align: center;
	}

	nav {
	    float: left;
	    padding-top: 3%;
	    display: block;
	}

	article {
		overflow: hidden;
		padding-left: 5%;
	}

	a:link, a:visited {
		color: #047DF2;/*#2da38d;#356d63;#3a8c7d;*/
		text-decoration: none;
	}

	strong {
	font-family: Lora;
	font-size: 16px;
	font-weight: 600;
}

</style>
</head>

<body>
    <div height="40" id="header" style="background-color:#008080; color: #008080">
      <center>
        <table width="1050" height="40" border="0">
          <tr>
		<td halign="center">
			<p align="center"><font size="6"><font color=#FFFFFF>Shuwen Qiu</font> </p>
		</td>
	</tr>
        </table>
      </center>
    </div>
	

	<nav class="nav">
		<center>
<!-- 			<a href="map.html"> -->
			<img src="images/janet2.JPG" style="height:200px;"></a>
		</center>
		<ul class="fa-ul">
			<li><i class="fa-li fa fa-envelope"></i>
				<a href="mailto:janetqiu@cs.ucla.edu" style="text-decoration: none">Email</a>
			</li>
			
			<li><i class="fa-li fa fa-github"></i>
				<a href="https://github.com/Janetalready/" style="text-decoration: none">Github</a>
			</li>
			
			<li><i class="fa-li fa fa-file"></i><a href="files/firstlast_cv.pdf">Resume</a>
			</li>
		</ul>
	</nav>

	<article class="article">
		<h1>About Me</h1>
		<p>I'm a fourth-year Ph.D. student in the Department of Computer Science at University of California, Los Angeles (UCLA). 
			I work at the <a href="https://vcla.stat.ucla.edu/">Center for Vision, Cognition, Learning and Autonomy (VCLA)</a> 
			under the supervision of <a href="http://www.stat.ucla.edu/~sczhu/">Prof. Song-Chun Zhu</a>.
			My research interests include social robots, cognitive modeling, human communication dynamics, etc.
	        </p>
		
<!-- 		<p><i> "Nothing in life is to be feared; it is only to be understood."</i>  --Marie Curie </p> -->
		
		

		<!-- <h1>News</h1>
		<p>03/2021: Our paper got accepted as Oral in CVPR 2021.
		<p>11/2019: I gave a poster presentation at ICCV 2019, Seoul, Korea.
		<p>09/2019: I gave a poster presentation at MURI 2019, Edinburgh. -->
<!-- 		<p>07/2019: One paper got accepted by ICCV 2019. -->
		<!-- <p>06/2018: I gave a spotlight presentation at the 4th Vision Meets Cognition workshop, CVPR 2018, Salt Lake City. <br>
        <p>03/2018: I advanced to candidacy. -->
<!--                 <p>02/2018: Our paper got accepted by CVPR 2018. -->
<!--                 <p>06/2017: I was selected as the 2016-2017 Most Promising Computational Statistician by the Statistics Department. -->
<!--                 <p>06/2017: Our CogSci 2017 paper received Computational Modeling Prize in Perception and Action. -->
<!--                 <p>04/2017: One paper was accepted for oral presentation at CogSci 2017. -->
        <!-- <p>09/2016: I started my Ph.D. life at UCLA.  -->
			
		<h1>Publication / Preprints</h1>
		<table>
		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/mindial/mindial.png" alt="" width="100%" class="border">
		   </td> 
		   <td width="80%" valign="center">
		   <papertitle>MindDial: Belief Dynamics Tracking with Theory-of-Mind Modeling for Neural Dialogue Generation</papertitle><br/>
		   <a href="https://janetalready.github.io/", style="color : #000000;"><strong>Shuwen Qiu</strong></a>,
		   <a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu</a>, 
		   <a href="http://web.cs.ucla.edu/~zilongzheng/", style="color : #000000;">Zilong Zheng</a>
	        </b><br/>
	           arXiv preprint arXiv:2306.15253 2023.<br/>
		    <a href="https://arxiv.org/abs/2306.15253">arXiv</a> 
		   </td>
		   </tr>
	       </table>	


		<table>
		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/pictionary/pictionary.gif" alt="" width="100%" class="border">
		   </td> 
		   <td width="80%" valign="center">
		   <papertitle>Emergent Graphical Conventions in a Visual Communication Game</papertitle><br/>
		   <a href="https://janetalready.github.io/", style="color : #000000;"><strong>Shuwen Qiu</strong></a>*,
		   <a href="https://siruixie.com/", style="color : #000000;">Sirui Xie</a>*,
	           <a href="https://lifengfan.github.io/", style="color : #000000;">Lifeng Fan</a>,
	           <a href="http://www.stat.ucla.edu/~taogao/", style="color : #000000;">Tao Gao</a>,
	           <a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu</a>, 
		   <a href="https://yzhu.io/", style="color : #000000;">Yixin Zhu</a>
		   </b><br/>	    
		   * Equal contributions  <br />
	            Conference on Neural Information Processing Systems (NeurIPS), 2022.<br/>
		    <a href="https://arxiv.org/abs/2111.14210">arXiv</a> /
		    <a href="https://sites.google.com/view/emergent-graphical-conventions">Web</a><br/>	
		   </td>
		   </tr>
	       </table>	

		<table>
		<table width="100%" border="0" align="top" cellpadding="10">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/acl21/arc.png" alt="" width="100%" class="border">
		   </td> 
		   <td width="80%" valign="center">
		   <papertitle>GRICE: A Grammar-based Dataset for Recovering Implicature and Conversational rEasoning</papertitle><br/>
		   <a href="http://web.cs.ucla.edu/~zilongzheng/", style="color : #000000;">Zilong Zheng</a>,
		   <a href="https://janetalready.github.io/", style="color : #000000;"><strong>Shuwen Qiu</strong></a>,
	           <a href="https://lifengfan.github.io/", style="color : #000000;">Lifeng Fan</a>,
		   <a href="https://yzhu.io/", style="color : #000000;">Yixin Zhu</a>,
	           <a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu</a> 
	           </b><br/>
	            ACL-Findings, 2021.<br/>
		    <a href="https://aclanthology.org/2021.findings-acl.182.pdf">Paper</a> /
		    <a href="https://zilongzheng.github.io/Grice/">Project</a> /
	            <a href="files/acl-f/cite.bib">Bibtex</a><br/>	
		   </td>
		   </tr>
	       </table>	
		
	      <table>
		<table width="100%" border="0" align="top" cellpadding="5">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/cvpr21/motivation.png" alt="belief dynamics" width="100%" class="border">
		   </td> 
		   <td width="80%" valign="center">
		   <papertitle>Learning Triadic Belief Dynamics in Nonverbal Communication from Videos</papertitle><br/>
	           <a href="https://lifengfan.github.io/", style="color : #000000;">Lifeng Fan</a>*,
                   <a href="https://janetalready.github.io/", style="color : #000000;"><strong>Shuwen Qiu</strong></a>*,
	           <a href="http://web.cs.ucla.edu/~zilongzheng/", style="color : #000000;">Zilong Zheng</a>,
	           <a href="http://www.stat.ucla.edu/~taogao/", style="color : #000000;">Tao Gao</a>,
	           <a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu</a>,
	           <a href="https://yzhu.io/", style="color : #000000;">Yixin Zhu</a>
		    </b><br/>	    
		   * Equal contributions  <br />
	            IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. (Oral) <br/>
		    <a href="files/cvpr21/TBD_paper.pdf">Paper</a> /
	            <a href="files/cvpr21/TBD_supp.pdf">Supp</a> /
	            <a href="https://github.com/LifengFan/Triadic-Belief-Dynamics">Code</a> /
		    <a href="https://docs.google.com/forms/d/e/1FAIpQLSe3v-qopGWjx3ZcrCzp7ReRf7VadBuVMhMXCsMe1z3qFVcGvA/viewform?usp=pp_url">Dataset</a> /
		    <a href="https://www.dropbox.com/s/nqai1z32bi66zuy/04411-video.mp4?dl=0">Demo</a> /
	            <a href="files/cvpr21/triadic_belief_dynamics.bib">Bibtex</a><br/>	
		   </td>
		   </tr>
	       </table>
	
		<table>
		<table width="100%" border="0" align="top" cellpadding="5">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/iros20/motivation.png" alt="ar workspace" width="100%" class="border">
		   </td> 
		   <td width="80%" valign="center">
		   <papertitle>Human-Robot Interaction in a Shared Augmented Reality Workspace</papertitle><br/>
	            <a href="https://janetalready.github.io/", style="color : #000000;"><strong>Shuwen Qiu</strong></a>*,
	            <a href="https://liuhx111.github.io/", style="color : #000000;">Hangxin Liu</a>*,
	            <a href="https://zeyuzhang.com/", style="color : #000000;">Zeyu Zhang</a>,
	            <a href="https://yzhu.io/", style="color : #000000;">Yixin Zhu</a>,
	            <a href="http://www.stat.ucla.edu/~sczhu/", style="color : #000000;">Song-Chun Zhu</a>
	            </b><br/>	    
		   * Equal contributions  <br />    
	            The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020. <br />
		    <a href="files/iros20/paper.pdf">Paper</a> /
		    <a href="https://github.com/Janetalready/Shared-AR-workspace">Code</a> /
		    <a href="https://sites.google.com/view/shared-ar-workspace/">Web</a> /
		    <a href="https://vimeo.com/439150333">Demo</a> /
		    <a href="https://vimeo.com/451048919">Presentation</a> /
	            <a href="files/iros20/cite.bib">Bibtex</a><br/>	
		   </td>
		   </tr>
	       </table>
	
	
	        <table>
		<table width="100%" border="0" align="top" cellpadding="5">
		   <tr>
		   <td width="20%" valign="center">
		   <img src="files/app20/motivation.png" alt="handshake" width="100%" class="border">
		   </td>
		   <td width="80%" valign="center">
		   <papertitle>Understanding the visual perception of awkward body movements: How interactions go awry</papertitle><br/>
	            <a href="https://www.researchgate.net/profile/Akila-Kadambi", style="color : #000000;">Akila Kadambi
</a>,
	            <a href="https://sites.google.com/view/nick-ichien/home", style="color : #000000;">Nick Ichien</a>,
	            <a href="https://janetalready.github.io/", style="color : #000000;"><strong>Shuwen Qiu</strong></a>,
	            <a href="http://cvl.psych.ucla.edu/", style="color : #000000;">Hongjing Lu</a>
		    </b><br/>	
		    Attention Perception & Psychophysics (APP), 2020. <br />
		    <a href="files/app20/paper.pdf">Paper</a>  /
     	            <a href="files/app20/cite.bib">Bibtex</a><br/>	
		   </td>
		   </tr>
	        </table>

			
	</article>
</body>
</html> 

